{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "froP-FoTlkiz",
    "outputId": "58e917cc-2ab4-4fbd-cfa8-e848fa4d7c29"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Kết nối với Google Drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5wcHjKyGOqJp",
    "outputId": "611db404-4898-4437-cac2-59b75e910109"
   },
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUWi6Qt0O1li"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Om9a4T2cO5Iw",
    "outputId": "11ae6a55-d934-4b90-c4bb-f68551ace088"
   },
   "outputs": [],
   "source": [
    "np.load(\"/content/gdrive/MyDrive/RL/Dichuyen/DaLoc/DaLoc.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jy5p0uyqO8KA",
    "outputId": "8ea7817e-41b5-4714-e17c-66880afc8320"
   },
   "outputs": [],
   "source": [
    "np.load(\"/content/gdrive/MyDrive/RL/Dichuyen/Reward/Reward.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "470G2WcUmT0i",
    "outputId": "213040bd-60f3-4b47-d568-5867905bb361"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4LOp_QnmX3y",
    "outputId": "0eff1221-89cf-45f3-9073-99ff4e747607"
   },
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBhTn4JRmZ4C"
   },
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OOSUolQ2mbKi",
    "outputId": "8eebcd14-13a9-4509-9c9f-20c5f0487091"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LocationEnv(Env):\n",
    "    def __init__(self, npy_file_path, reward_file_path):\n",
    "        # Đọc dữ liệu từ file .npy (dữ liệu lộ trình)\n",
    "        self.route_data = np.load(npy_file_path)\n",
    "        self.current_index = 0  # Bắt đầu từ điểm đầu tiên trong lộ trình\n",
    "\n",
    "        # Đọc tọa độ mục tiêu từ file reward.npy\n",
    "        self.target_location = np.load(reward_file_path)\n",
    "\n",
    "        # Action space: 3 hành động - Lùi lại, Tiến lên hoặc Giữ nguyên\n",
    "        self.action_space = Discrete(3)  # 0: Lùi lại, 1: Tiến lên, 2: Giữ nguyên\n",
    "\n",
    "        # Observation space: dựa trên giá trị kinh độ và vĩ độ từ dữ liệu\n",
    "        low = np.min(self.route_data, axis=0)\n",
    "        high = np.max(self.route_data, axis=0)\n",
    "        self.observation_space = Box(low=low, high=high, dtype=np.float32)\n",
    "\n",
    "        # Trạng thái ban đầu\n",
    "        self.state = np.copy(self.route_data[self.current_index])\n",
    "\n",
    "    def reset(self):\n",
    "        # Đặt lại trạng thái về vị trí bắt đầu\n",
    "        self.current_index = 0\n",
    "        self.state = np.copy(self.route_data[self.current_index])\n",
    "        return self.state.flatten()  # Trả về trạng thái phẳng\n",
    "\n",
    "    def step(self, action):\n",
    "        # Xử lý hành động\n",
    "        if action == 1:  # Tiến lên\n",
    "            if self.current_index < len(self.route_data) - 1:\n",
    "                self.current_index += 1\n",
    "        elif action == 0:  # Lùi lại\n",
    "            if self.current_index > 0:\n",
    "                self.current_index -= 1\n",
    "        elif action == 2:  # Giữ nguyên\n",
    "            pass  # Không thay đổi vị trí\n",
    "\n",
    "        # Cập nhật trạng thái\n",
    "        self.state = np.copy(self.route_data[self.current_index])\n",
    "\n",
    "        # Tính phần thưởng dựa trên khoảng cách đến mục tiêu\n",
    "        distance = np.linalg.norm(self.state - self.target_location)\n",
    "        reward = -distance  # Phần thưởng âm theo khoảng cách, càng gần mục tiêu càng tốt\n",
    "\n",
    "        # Kiểm tra nếu agent rất gần mục tiêu\n",
    "        if distance < 0.001:\n",
    "            done = True\n",
    "            reward = 10  # Phần thưởng cao khi đạt gần mục tiêu\n",
    "        else:\n",
    "            done = self.current_index == len(self.route_data) - 1\n",
    "\n",
    "        # Thông tin bổ sung\n",
    "        info = {\"distance\": distance}\n",
    "\n",
    "        return self.state.flatten(), reward, done, info  # Trả về trạng thái phẳng\n",
    "\n",
    "    def render(self):\n",
    "        # Hiển thị trạng thái hiện tại của môi trường\n",
    "        print(f\"Current State: {self.state}, Current Index: {self.current_index}\")\n",
    "\n",
    "        # Vẽ các điểm GPS trên bản đồ 2D sử dụng matplotlib\n",
    "        plt.plot(self.route_data[:, 0], self.route_data[:, 1], label='Route', color='blue')\n",
    "        plt.scatter(self.state[0], self.state[1], color='red', label='Current Location')\n",
    "        plt.scatter(self.target_location[0], self.target_location[1], color='green', label='Target Location')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        plt.title('GPS Path Visualization')\n",
    "        plt.show()\n",
    "\n",
    "# Khởi tạo môi trường và huấn luyện như trước\n",
    "npy_file_path = r\"/content/gdrive/MyDrive/RL/Dichuyen/DaLoc/DaLoc.npy\"\n",
    "reward_file_path = r\"/content/gdrive/MyDrive/RL/Dichuyen/Reward/Reward.npy\"\n",
    "env = LocationEnv(npy_file_path, reward_file_path)\n",
    "\n",
    "# Khởi tạo Q-learning agent và bắt đầu huấn luyện\n",
    "agent = QLearningAgent(env)\n",
    "agent.train(episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFupAJ1WGVQf"
   },
   "outputs": [],
   "source": [
    "npy_file_path = r\"/content/gdrive/MyDrive/RL/Dichuyen/DaLoc/DaLoc.npy\"\n",
    "reward_file_path = r\"/content/gdrive/MyDrive/RL/Dichuyen/Reward/Reward.npy\"\n",
    "env = LocationEnv(npy_file_path, reward_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dlp9XxSTRjPC"
   },
   "outputs": [],
   "source": [
    "!pip instal matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "1evHvRS-RmKg",
    "outputId": "0452c06f-6d9c-477d-93e0-667fbda342ed"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_min = epsilon_min  # Minimum exploration rate\n",
    "        self.epsilon_decay = epsilon_decay  # Epsilon decay factor\n",
    "\n",
    "        # Khởi tạo Q-table với giá trị 0 cho tất cả các trạng thái và hành động\n",
    "        self.q_table = np.zeros((len(env.route_data), env.action_space.n))\n",
    "\n",
    "        # Biến để lưu điểm số mỗi episode\n",
    "        self.scores = []\n",
    "\n",
    "    def choose_action(self, state_index):\n",
    "        # Epsilon-greedy strategy: chọn ngẫu nhiên hoặc chọn hành động tối ưu từ Q-table\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample()  # Chọn hành động ngẫu nhiên (explore)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state_index])  # Chọn hành động tối ưu (exploit)\n",
    "\n",
    "    def learn(self, state_index, action, reward, next_state_index):\n",
    "        # Cập nhật Q-value theo công thức Q-learning\n",
    "        predict = self.q_table[state_index, action]\n",
    "        target = reward + self.gamma * np.max(self.q_table[next_state_index])\n",
    "        self.q_table[state_index, action] += self.alpha * (target - predict)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        # Giảm dần epsilon để giảm hành động ngẫu nhiên theo thời gian\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            state_index = self.env.current_index\n",
    "            done = False\n",
    "            score = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.choose_action(state_index)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                next_state_index = self.env.current_index\n",
    "\n",
    "                # Học từ kinh nghiệm\n",
    "                self.learn(state_index, action, reward, next_state_index)\n",
    "\n",
    "                # Cập nhật trạng thái\n",
    "                state_index = next_state_index\n",
    "                score += reward\n",
    "\n",
    "            # Lưu lại điểm số sau mỗi episode\n",
    "            self.scores.append(score)\n",
    "\n",
    "            # Giảm epsilon sau mỗi episode\n",
    "            self.decay_epsilon()\n",
    "\n",
    "            print(f\"Episode: {episode + 1}, Score: {score}, Epsilon: {self.epsilon:.3f}\")\n",
    "\n",
    "        print(\"Training finished.\")\n",
    "\n",
    "        # Plotting the score progress\n",
    "        self.plot_scores()\n",
    "\n",
    "    def plot_scores(self):\n",
    "        # Vẽ biểu đồ điểm số qua từng episode\n",
    "        plt.plot(self.scores)\n",
    "        plt.title(\"QLearning Agent Performance\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "# Khởi tạo môi trường và agent sau khi đã sửa đổi\n",
    "env = LocationEnv(npy_file_path, reward_file_path)\n",
    "agent = QLearningAgent(env)\n",
    "agent.train(episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPy3oF2Cmpjr",
    "outputId": "04db84fa-3f49-40b4-9e60-822c954c2d52"
   },
   "outputs": [],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2-eF0-MbAkI0",
    "outputId": "017bdc80-53d8-4a67-9088-dd11e27bce8d"
   },
   "outputs": [],
   "source": [
    "print(env.observation_space.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7yeUevVpmxQD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRKdRQBCmzYa"
   },
   "outputs": [],
   "source": [
    "\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syw2Sb0ySk1Z"
   },
   "outputs": [],
   "source": [
    "input_shape = env.observation_space.shape\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=input_shape))  # Ensure the input is flattened\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(env.action_space.n, activation='linear'))  # Output layer for actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NYbrItum0pS",
    "outputId": "2872f536-380d-4f03-a018-7aecfb039819"
   },
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qtn1IeTqm2KL"
   },
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, activation='relu', input_shape=(states,)))  # Vẫn giữ nguyên\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTc9oU9RAzjU",
    "outputId": "6b0a7b0a-95c8-4f4f-8cac-b0f012134ee4"
   },
   "outputs": [],
   "source": [
    "!pip install keras-rl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JDftp8UQBKlj",
    "outputId": "fbd5296e-b8cc-4e69-e53a-5389ffbc1dc8"
   },
   "outputs": [],
   "source": [
    "!pip install keras-rl2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "gm7qtp3zEYMj",
    "outputId": "ff15ed15-7212-4719-a1d8-fd7293eff4b8"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "import numpy as np\n",
    "\n",
    "# Giả sử bạn đã có môi trường 'env' sẵn sàng\n",
    "\n",
    "# Hàm xây dựng mô hình\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, states)))  # Flatten input (1, states)\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))  # Output layer for actions\n",
    "    return model\n",
    "\n",
    "# Kích thước không gian quan sát và không gian hành động\n",
    "states = np.prod(env.observation_space.shape)  # Number of state values (flattened size)\n",
    "actions = env.action_space.n  # Number of actions\n",
    "\n",
    "# Xây dựng mô hình\n",
    "model = build_model(states, actions)\n",
    "\n",
    "# Khởi tạo bộ nhớ\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "\n",
    "# Chính sách BoltzmannQPolicy\n",
    "policy = BoltzmannQPolicy()\n",
    "\n",
    "# Khởi tạo agent DQN\n",
    "dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "               nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "\n",
    "# Compile the DQN agent\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Huấn luyện agent và hiển thị quá trình học\n",
    "import gym\n",
    "\n",
    "# Tạo môi trường với render_mode phù hợp\n",
    "env = gym.make('LocationEnv', render_mode='human')  # 'human' để hiển thị đồ họa\n",
    "\n",
    "# Tiếp tục với quá trình huấn luyện\n",
    "dqn.fit(env, nb_steps=50000, visualize=True, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Za6Ei3vUm4Mk"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hVCmsNnim5cD",
    "outputId": "d3d176b2-1e46-4e80-81f7-38f46af18b48"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OT4QBbII9CV"
   },
   "outputs": [],
   "source": [
    "# Sau khi huấn luyện hoàn tất\n",
    "# Giả sử bạn đã huấn luyện mô hình Q-learning trong phần trước\n",
    "agent.train(episodes=100)\n",
    "\n",
    "# Sau khi huấn luyện, bạn có thể lưu trữ Q-table\n",
    "np.save('q_table.npy', agent.q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyU1ub7YJBxH"
   },
   "outputs": [],
   "source": [
    "# Lưu mô hình DQN\n",
    "dqn.model.save('dqn_model.h5')  # Lưu toàn bộ mô hình (weights + architecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6TihtnqJEbW"
   },
   "outputs": [],
   "source": [
    "# Tải lại Q-table\n",
    "q_table = np.load('q_table.npy')\n",
    "\n",
    "# Sử dụng mô hình đã huấn luyện trong ứng dụng\n",
    "# Chọn hành động từ Q-table cho một trạng thái cụ thể\n",
    "state_index = 10  # Ví dụ về một trạng thái\n",
    "action = np.argmax(q_table[state_index])  # Chọn hành động tối ưu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlAndYT2JHot"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "# Giả sử bạn đã tải mô hình đã huấn luyện\n",
    "model = load_model('dqn_model.h5')\n",
    "\n",
    "# Hàm quyết định hành động trong ứng dụng chỉ đường\n",
    "def choose_action(state):\n",
    "    # Dự đoán hành động từ mô hình DQN\n",
    "    action = np.argmax(model.predict(state.reshape(1, 1, 2)))  # Dự đoán hành động tối ưu\n",
    "    return action\n",
    "\n",
    "npy_file_path = r\"/content/gdrive/MyDrive/RL/Dichuyen/DaLoc/DaLoc.npy\"\n",
    "reward_file_path = r\"/content/gdrive/MyDrive/RL/Dichuyen/Reward/Reward.npy\"\n",
    "# Đọc tọa độ từ file .npy\n",
    "user_positions = np.load(npy_file_path)  # Mảng (n, 2), mỗi dòng là [longitude, latitude]\n",
    "destination_positions = np.load(reward_file_path)  # Mảng (m, 2), mỗi dòng là [longitude, latitude]\n",
    "\n",
    "# Giả sử bạn muốn tính khoảng cách đến từng điểm đích\n",
    "for current_position in user_positions:\n",
    "    print(f\"Current Position: {current_position}\")\n",
    "\n",
    "    # Chọn hành động dựa trên mô hình đã huấn luyện\n",
    "    action = choose_action(current_position)\n",
    "\n",
    "    # Thực hiện hành động trong ứng dụng (ví dụ: di chuyển đến điểm tiếp theo)\n",
    "    # Cập nhật vị trí người dùng sau khi hành động\n",
    "    if action == 0:  # Ví dụ: di chuyển về phía trước (tiến lên)\n",
    "        current_position += [0.0001, 0.0001]  # Cập nhật vị trí mới (di chuyển lên trên)\n",
    "    elif action == 1:  # Quay trái (có thể thay bằng hành động quay trái thực tế)\n",
    "        current_position += [-0.0001, 0.0001]  # Quay trái (giảm longitude, tăng latitude)\n",
    "    elif action == 2:  # Quay phải (có thể thay bằng hành động quay phải thực tế)\n",
    "        current_position += [0.0001, -0.0001]  # Quay phải (tăng longitude, giảm latitude)\n",
    "\n",
    "    # Hiển thị vị trí mới trên bản đồ hoặc UI của ứng dụng\n",
    "    print(f\"New Position: {current_position}\")\n",
    "\n",
    "    # (Tùy chọn) Kiểm tra khoảng cách đến từng điểm đích\n",
    "    for destination_position in destination_positions:\n",
    "        distance_to_target = np.linalg.norm(current_position - destination_position)\n",
    "        print(f\"Distance to target: {distance_to_target:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10a5J_aiO1oc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Giả sử bạn đã tải mô hình đã huấn luyện\n",
    "model = load_model('dqn_model.h5')\n",
    "\n",
    "# Hàm quyết định hành động trong ứng dụng chỉ đường\n",
    "def choose_action(state):\n",
    "    # Dự đoán hành động từ mô hình DQN\n",
    "    action = np.argmax(model.predict(state.reshape(1, 1, 2)))  # Dự đoán hành động tối ưu\n",
    "    return action\n",
    "\n",
    "# Đọc tọa độ từ file .npy\n",
    "npy_file_path = r\"/content/gdrive/MyDrive/RL/Dichuyen/DaLoc/DaLoc.npy\"\n",
    "reward_file_path = r\"/content/gdrive/MyDrive/RL/Dichuyen/Reward/Reward.npy\"\n",
    "\n",
    "# Đọc tọa độ từ file .npy\n",
    "user_positions = np.load(npy_file_path)  # Mảng (n, 2), mỗi dòng là [longitude, latitude]\n",
    "destination_positions = np.load(reward_file_path)  # Mảng (m, 2), mỗi dòng là [longitude, latitude]\n",
    "\n",
    "# Tạo danh sách để lưu các tọa độ dự đoán\n",
    "predicted_positions = []\n",
    "\n",
    "# Giả sử bạn muốn tính khoảng cách đến từng điểm đích\n",
    "for current_position in user_positions:\n",
    "    print(f\"Current Position: {current_position}\")\n",
    "\n",
    "    # Chọn hành động dựa trên mô hình đã huấn luyện\n",
    "    action = choose_action(current_position)\n",
    "\n",
    "    # Thực hiện hành động trong ứng dụng (ví dụ: di chuyển đến điểm tiếp theo)\n",
    "    # Cập nhật vị trí người dùng sau khi hành động\n",
    "    if action == 0:  # Di chuyển về phía trước (tiến lên)\n",
    "        current_position += [0.0001, 0.0001]  # Cập nhật vị trí mới (di chuyển lên trên)\n",
    "    elif action == 1:  # Quay trái\n",
    "        current_position += [-0.0001, 0.0001]  # Quay trái (giảm longitude, tăng latitude)\n",
    "    elif action == 2:  # Quay phải\n",
    "        current_position += [0.0001, -0.0001]  # Quay phải (tăng longitude, giảm latitude)\n",
    "\n",
    "    # Lưu vị trí dự đoán vào danh sách\n",
    "    predicted_positions.append(current_position.copy())\n",
    "\n",
    "    # Hiển thị vị trí mới trên bản đồ hoặc UI của ứng dụng\n",
    "    print(f\"New Position: {current_position}\")\n",
    "\n",
    "# Chuyển predicted_positions thành mảng numpy để dễ dàng vẽ\n",
    "predicted_positions = np.array(predicted_positions)\n",
    "\n",
    "# Vẽ bản đồ và các điểm\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Vẽ các điểm GPS thực tế (màu xanh)\n",
    "plt.scatter(user_positions[:, 0], user_positions[:, 1], c='blue', label='Real GPS', marker='o')\n",
    "\n",
    "# Vẽ các điểm GPS dự đoán (màu đỏ)\n",
    "plt.scatter(predicted_positions[:, 0], predicted_positions[:, 1], c='red', label='Predicted GPS', marker='x')\n",
    "\n",
    "# Vẽ các điểm Reward (màu xanh lá cây)\n",
    "plt.scatter(destination_positions[:, 0], destination_positions[:, 1], c='green', label='Reward (Destination)', marker='^')\n",
    "\n",
    "# Thêm các đường nối giữa các điểm thực tế và dự đoán\n",
    "for i in range(len(user_positions)):\n",
    "    plt.plot([user_positions[i, 0], predicted_positions[i, 0]],\n",
    "             [user_positions[i, 1], predicted_positions[i, 1]],\n",
    "             'gray', linestyle='--')\n",
    "\n",
    "# Kiểm tra khoảng cách giữa các điểm dự đoán và các điểm Reward (tọa độ đích)\n",
    "for i, predicted_position in enumerate(predicted_positions):\n",
    "    # Tính khoảng cách giữa điểm dự đoán và tất cả các điểm đích\n",
    "    distances = np.linalg.norm(destination_positions - predicted_position, axis=1)\n",
    "\n",
    "    # Tìm chỉ số của điểm đích gần nhất\n",
    "    closest_index = np.argmin(distances)\n",
    "    closest_distance = distances[closest_index]\n",
    "\n",
    "    # Đánh dấu điểm dự đoán gần nhất với điểm đích\n",
    "    if closest_distance < 0.001:  # Nếu khoảng cách nhỏ hơn 0.001, đánh dấu điểm\n",
    "        plt.scatter(predicted_position[0], predicted_position[1], c='yellow', marker='*', s=200, label='Closest to Reward' if i == 0 else \"\")\n",
    "\n",
    "# Thêm tiêu đề và nhãn cho trục\n",
    "plt.title(\"Real GPS vs Predicted GPS Positions with Reward\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "# Thêm chú thích\n",
    "plt.legend()\n",
    "\n",
    "# Hiển thị bản đồ\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1SYU6lvm8RO",
    "outputId": "be7871d1-7664-473b-af79-22e5e5101a4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/usr/lib/python3.10/random.py:370: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version\n",
      "  return self.randrange(a, b+1)\n",
      "/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "input_shape = env.observation_space.shape\n",
    "def build_model(states, actions):\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape))  # Ensure the input is flattened\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(env.action_space.n, activation='linear'))  # Output layer for actions\n",
    "    return model\n",
    "# Kích thước không gian quan sát và không gian hành động\n",
    "states = np.prod(env.observation_space.shape)  # Số lượng giá trị trong trạng thái (2)\n",
    "actions = env.action_space.n  # Số lượng hành động (2 hành động: tiến và lùi)\n",
    "\n",
    "# Xây dựng mô hình\n",
    "model = build_model(states, actions)\n",
    "\n",
    "# Khởi tạo bộ nhớ\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "\n",
    "# Chính sách sử dụng BoltzmannQPolicy\n",
    "policy = BoltzmannQPolicy()\n",
    "\n",
    "# Khởi tạo agent DQN\n",
    "dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "               nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1hhCJ7cS__4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten  # Import Flatten\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# Function to build the model\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, states)))  # Flatten input (1, states) because DQN expects batch size in first dimension\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))  # Output layer for actions\n",
    "    return model  # Return the model\n",
    "\n",
    "# Kích thước không gian quan sát và không gian hành động\n",
    "states = np.prod(env.observation_space.shape)  # Number of state values (flattened size)\n",
    "actions = env.action_space.n  # Number of actions (2 actions: forward and backward)\n",
    "\n",
    "# Xây dựng mô hình\n",
    "model = build_model(states, actions)\n",
    "\n",
    "# Khởi tạo bộ nhớ\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "\n",
    "# Chính sách sử dụng BoltzmannQPolicy\n",
    "policy = BoltzmannQPolicy()\n",
    "\n",
    "# Khởi tạo agent DQN\n",
    "dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "               nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "\n",
    "# Compile the DQN agent\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Start training\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MPZ8CEmceXY"
   },
   "source": [
    "Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjtkHgp6ceI4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent  # DQN agent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# Function to build the model\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, states)))  # Flatten input (1, states) because DQN expects batch size in the first dimension\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))  # Output layer for actions\n",
    "    return model  # Return the model\n",
    "\n",
    "# Kích thước không gian quan sát và không gian hành động\n",
    "states = np.prod(env.observation_space.shape)  # Number of state values (flattened size)\n",
    "actions = env.action_space.n  # Number of actions (2 actions: forward and backward)\n",
    "\n",
    "# Xây dựng mô hình online network (chính)\n",
    "model = build_model(states, actions)\n",
    "\n",
    "# Khởi tạo bộ nhớ\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "\n",
    "# Chính sách sử dụng BoltzmannQPolicy\n",
    "policy = BoltzmannQPolicy()\n",
    "\n",
    "# Khởi tạo agent Double DQN\n",
    "dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "               nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2,\n",
    "               enable_double_dqn=True)  # Sử dụng Double DQN bằng cách bật cờ enable_double_dqn\n",
    "\n",
    "# Compile the DQN agent\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Start training with Double DQN\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUMsQTU2R38A"
   },
   "outputs": [],
   "source": [
    "# Huấn luyện agent\n",
    "# Huấn luyện agent\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)\n",
    "\n",
    "\n",
    "# Đánh giá agent sau khi huấn luyện\n",
    "dqn.test(env, nb_episodes=5, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snhHQbHFjvJR"
   },
   "source": [
    "DDPQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuRVw_2Tjuu5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Input\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "from keras.layers import Concatenate\n",
    "from gym.spaces import Box\n",
    "\n",
    "class LocationEnv(Env):\n",
    "    def __init__(self, npy_file_path, reward_file_path):\n",
    "        # Đọc dữ liệu từ file .npy (dữ liệu lộ trình)\n",
    "        self.route_data = np.load(npy_file_path)\n",
    "        self.current_index = 0  # Bắt đầu từ điểm đầu tiên trong lộ trình\n",
    "\n",
    "        # Đọc tọa độ mục tiêu từ file reward.npy\n",
    "        self.target_location = np.load(reward_file_path)\n",
    "\n",
    "        # Action space: Continuous action space\n",
    "        self.action_space = Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "\n",
    "        # Observation space: dựa trên giá trị kinh độ và vĩ độ từ dữ liệu\n",
    "        low = np.min(self.route_data, axis=0)\n",
    "        high = np.max(self.route_data, axis=0)\n",
    "        self.observation_space = Box(low=low, high=high, dtype=np.float32)\n",
    "\n",
    "        # Trạng thái ban đầu\n",
    "        self.state = np.copy(self.route_data[self.current_index])\n",
    "\n",
    "    def reset(self):\n",
    "        # Đặt lại trạng thái về vị trí bắt đầu\n",
    "        self.current_index = 0\n",
    "        self.state = np.copy(self.route_data[self.current_index])\n",
    "        return self.state.flatten()  # Trả về trạng thái phẳng\n",
    "\n",
    "    def step(self, action):\n",
    "        # Xử lý hành động\n",
    "        action = action[0]\n",
    "        max_step_size = 1  # Thiết lập kích thước bước tối đa\n",
    "        if action > 0:  # Tiến lên\n",
    "            step_size = int(np.clip(action * max_step_size, 1, max_step_size))\n",
    "            self.current_index = min(self.current_index + step_size, len(self.route_data) - 1)\n",
    "        elif action < 0:  # Lùi lại\n",
    "            step_size = int(np.clip(-action * max_step_size, 1, max_step_size))\n",
    "            self.current_index = max(self.current_index - step_size, 0)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # Cập nhật trạng thái\n",
    "        self.state = np.copy(self.route_data[self.current_index])\n",
    "\n",
    "        # Tính phần thưởng dựa trên khoảng cách đến mục tiêu\n",
    "        distance = np.linalg.norm(self.state - self.target_location)\n",
    "        reward = -distance  # Phần thưởng âm theo khoảng cách, càng gần mục tiêu càng tốt\n",
    "\n",
    "        # Kiểm tra nếu agent rất gần mục tiêu\n",
    "        if distance < 0.001:\n",
    "            done = True\n",
    "            reward = 10  # Phần thưởng cao khi đạt gần mục tiêu\n",
    "        else:\n",
    "            done = self.current_index == len(self.route_data) - 1\n",
    "\n",
    "        # Thông tin bổ sung\n",
    "        info = {\"distance\": distance}\n",
    "\n",
    "        return self.state.flatten(), reward, done, info  # Trả về trạng thái phẳng\n",
    "\n",
    "    def render(self):\n",
    "        # Hiển thị trạng thái hiện tại của môi trường\n",
    "        print(f\"Current State: {self.state}, Current Index: {self.current_index}\")\n",
    "\n",
    "\n",
    "# Xây dựng mạng Actor\n",
    "def build_actor(states, actions):\n",
    "    actor = Sequential()\n",
    "    actor.add(Flatten(input_shape=(1, states)))  # Flatten input\n",
    "    actor.add(Dense(24, activation='relu'))\n",
    "    actor.add(Dense(24, activation='relu'))\n",
    "    actor.add(Dense(actions, activation='tanh'))  # Continuous action space output\n",
    "    return actor\n",
    "\n",
    "# Xây dựng mạng Critic\n",
    "def build_critic(states, actions):\n",
    "    # Đầu vào quan sát (trạng thái)\n",
    "    observation_input = Input(shape=(1, states), name='observation_input')\n",
    "    flattened_observation = Flatten()(observation_input)\n",
    "\n",
    "    # Đầu vào hành động\n",
    "    action_input = Input(shape=(actions,), name='action_input')\n",
    "\n",
    "    # Kết hợp (concatenate) giữa trạng thái và hành động\n",
    "    concatenated = Concatenate()([flattened_observation, action_input])\n",
    "\n",
    "    x = Dense(24, activation='relu')(concatenated)\n",
    "    x = Dense(24, activation='relu')(x)\n",
    "    q_output = Dense(1, activation='linear')(x)  # Output Q-value\n",
    "\n",
    "    # Tạo mô hình Critic\n",
    "    critic = Model(inputs=[observation_input, action_input], outputs=q_output)\n",
    "    return critic\n",
    "\n",
    "\n",
    "# Đường dẫn đến các file dữ liệu\n",
    "npy_file_path = r\"/content/gdrive/MyDrive/RL/Dichuyen/DaLoc/DaLoc.npy\"\n",
    "reward_file_path = r\"/content/gdrive/MyDrive/RL/Dichuyen/Reward/Reward.npy\"\n",
    "\n",
    "# Khởi tạo môi trường\n",
    "env = LocationEnv(npy_file_path, reward_file_path)\n",
    "\n",
    "# Kích thước không gian quan sát và hành động\n",
    "states = np.prod(env.observation_space.shape)\n",
    "actions = env.action_space.shape[0]\n",
    "\n",
    "# Xây dựng các mạng Actor và Critic\n",
    "actor = build_actor(states, actions)\n",
    "critic = build_critic(states, actions)\n",
    "\n",
    "# Khởi tạo bộ nhớ\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "\n",
    "# Sử dụng Ornstein-Uhlenbeck process để thêm noise vào các hành động\n",
    "random_process = OrnsteinUhlenbeckProcess(size=actions, theta=0.15, mu=0.0, sigma=0.2)\n",
    "\n",
    "# Khởi tạo DDPG agent\n",
    "ddpg = DDPGAgent(actor=actor, critic=critic, critic_action_input=critic.input[1],\n",
    "                 memory=memory, nb_actions=actions, nb_steps_warmup_actor=100,\n",
    "                 nb_steps_warmup_critic=100, random_process=random_process, gamma=0.99,\n",
    "                 target_model_update=1e-3)\n",
    "\n",
    "# Compile agent\n",
    "ddpg.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Huấn luyện agent\n",
    "ddpg.fit(env, nb_steps=50000, visualize=False, verbose=1)\n",
    "\n",
    "# Kiểm thử agent\n",
    "ddpg.test(env, nb_episodes=5, visualize=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiMKJ4o8rSvW"
   },
   "source": [
    "A3C - Asynchronous Advantage Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8D5YiaLlptOe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from gym import Env\n",
    "from gym.spaces import Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rZxEmTN5prCM",
    "outputId": "5cc70871-55b0-4a1a-d3a1-b2b8d3ab772f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution enabled: True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Eager execution enabled:\", tf.executing_eagerly())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2pCki7CErBU",
    "outputId": "5ddd3cf5-9d0c-4bc8-e930-1c51824714d0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from gym.spaces import Box, Discrete\n",
    "from gym import Env\n",
    "import matplotlib.pyplot as plt\n",
    "import threading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "4HtyHlgPDmAU",
    "outputId": "897fcc1b-4cfd-4291-d8a4-ac12592f31ca"
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "AAPvRpcS-aMW",
    "outputId": "2a04ebdf-6ed5-4c1d-a627-c9d694a11cc8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wJFgEK5En3a"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "IxcjkhWf_Ii2",
    "outputId": "9d7fac04-7b85-42e8-d18a-ee16455092df"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Kiểm tra xem eager execution đã được bật chưa\n",
    "print(\"Eager execution enabled:\", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMn3oZNt-BQf",
    "outputId": "9d96813c-2b8f-41b9-cc76-f76a7a1f1719"
   },
   "outputs": [],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "ZGZAQzEzrcyG",
    "outputId": "46f86254-d909-4680-e253-40ba133600c6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import matplotlib.pyplot as plt\n",
    "import threading\n",
    "\n",
    "# Custom Environment\n",
    "class LocationEnv(Env):\n",
    "    def __init__(self, npy_file_path, reward_file_path):\n",
    "        self.route_data = np.load(npy_file_path)\n",
    "        self.target_location = np.load(reward_file_path)\n",
    "        self.current_index = 0\n",
    "\n",
    "        self.action_space = Discrete(3)  # 0: Move Backward, 1: Move Forward, 2: Stay in Place\n",
    "        low = np.min(self.route_data, axis=0)\n",
    "        high = np.max(self.route_data, axis=0)\n",
    "        self.observation_space = Box(low=low, high=high, dtype=np.float32)\n",
    "\n",
    "        self.state = np.copy(self.route_data[self.current_index])\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_index = 0\n",
    "        self.state = np.copy(self.route_data[self.current_index])\n",
    "        return np.expand_dims(np.copy(self.state), axis=0)  # Adding batch size\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 1:  # Move Forward\n",
    "            if self.current_index < len(self.route_data) - 1:\n",
    "                self.current_index += 1\n",
    "        elif action == 0:  # Move Backward\n",
    "            if self.current_index > 0:\n",
    "                self.current_index -= 1\n",
    "        elif action == 2:  # Stay in Place\n",
    "            pass\n",
    "\n",
    "        self.state = np.copy(self.route_data[self.current_index])\n",
    "\n",
    "        # Compute reward based on distance to the target location\n",
    "        distance = np.linalg.norm(self.state - self.target_location)\n",
    "        reward = -distance  # Negative reward for distance\n",
    "\n",
    "        # Check if agent is very close to the target\n",
    "        done = distance < 0.001 or self.current_index == len(self.route_data) - 1\n",
    "\n",
    "        if done:\n",
    "            reward = 10  # Reward for reaching the target\n",
    "\n",
    "        return np.expand_dims(np.copy(self.state), axis=0), reward, done, {\"distance\": distance}\n",
    "\n",
    "    def render(self):\n",
    "        # Display current state on a 2D map using matplotlib\n",
    "        plt.plot(self.route_data[:, 0], self.route_data[:, 1], label='Route', color='blue')\n",
    "        plt.scatter(self.state[0], self.state[1], color='red', label='Current Location')\n",
    "        plt.scatter(self.target_location[0], self.target_location[1], color='green', label='Target Location')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        plt.title('GPS Path Visualization')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Actor-Critic Network (Shared Network)\n",
    "class ActorCritic(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.shared = layers.Dense(128, activation=\"relu\")\n",
    "        self.actor = layers.Dense(action_size, activation=\"softmax\")  # Actor output (probabilities)\n",
    "        self.critic = layers.Dense(1)  # Critic output (value estimation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.shared(inputs)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "\n",
    "# Worker class for A3C (Asynchronous Advantage Actor-Critic)\n",
    "class Worker(threading.Thread):\n",
    "    def __init__(self, env, global_model, optimizer, gamma, worker_id, max_episodes, batch_size):\n",
    "        super(Worker, self).__init__()\n",
    "        self.env = env\n",
    "        self.global_model = global_model\n",
    "        self.optimizer = optimizer\n",
    "        self.gamma = gamma\n",
    "        self.worker_id = worker_id\n",
    "        self.max_episodes = max_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.local_model = ActorCritic(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "        # Ensure that the local model is built with correct input shape before copying weights\n",
    "        self.local_model.build(input_shape=(None, env.observation_space.shape[0]))  # Build model with correct input shape\n",
    "        self.local_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))  # Compile to initialize weights\n",
    "\n",
    "        # Ensure weights are initialized and then copy from global_model\n",
    "        if len(self.local_model.weights) == 0:\n",
    "            self.local_model.build(input_shape=(None, env.observation_space.shape[0]))  # Rebuild model if needed\n",
    "            print(f\"Local model weights were not initialized. Initializing them for worker {self.worker_id}.\")\n",
    "\n",
    "        # Before setting weights, check if the weights are initialized properly\n",
    "        if len(self.global_model.get_weights()) > 0:\n",
    "            self.local_model.set_weights(self.global_model.get_weights())  # Now it's safe to set weights\n",
    "        else:\n",
    "            print(f\"Global model weights are not initialized for worker {self.worker_id}.\")\n",
    "\n",
    "    def run(self):\n",
    "        global_episode = 0\n",
    "        while global_episode < self.max_episodes:\n",
    "            state = self.env.reset()\n",
    "            state = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "            memory = []\n",
    "            total_reward = 0\n",
    "\n",
    "            while True:\n",
    "                logits, value = self.local_model(state)\n",
    "\n",
    "                # Choose action based on probabilities from the actor\n",
    "                action_probabilities = tf.squeeze(logits, axis=0).numpy()\n",
    "                action = np.random.choice(len(action_probabilities), p=action_probabilities)\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                memory.append((state, action, reward, value))\n",
    "\n",
    "                state = tf.convert_to_tensor(next_state, dtype=tf.float32)\n",
    "                if done:\n",
    "                    self.update_global(memory, done)\n",
    "                    break\n",
    "\n",
    "            print(f\"Worker {self.worker_id}, Episode Reward: {total_reward}\")\n",
    "            global_episode += 1\n",
    "\n",
    "    def update_global(self, memory, done):\n",
    "        states, actions, rewards, values = zip(*memory)\n",
    "\n",
    "        # Apply batch size (use batch_size for each update)\n",
    "        returns = []\n",
    "        discounted_sum = 0 if done else values[-1].numpy()\n",
    "        for reward in reversed(rewards):\n",
    "            discounted_sum = reward + self.gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        returns = tf.convert_to_tensor(returns, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        values = tf.squeeze(tf.convert_to_tensor(values), axis=1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits, critic_values = self.local_model(tf.concat(states, axis=0))  # Concatenating states to form batch\n",
    "            critic_loss = tf.reduce_mean(tf.square(returns - critic_values))\n",
    "            actor_loss = -tf.reduce_mean((returns - critic_values) * logits)\n",
    "            total_loss = critic_loss + actor_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.local_model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.global_model.trainable_weights))\n",
    "\n",
    "        # Update global model with the local model's weights after training\n",
    "        self.global_model.set_weights(self.local_model.get_weights())\n",
    "\n",
    "\n",
    "# Main A3C Training\n",
    "def train_a3c(env_path, reward_path, max_episodes=500, num_workers=4, gamma=0.99, batch_size=32):\n",
    "    env = LocationEnv(env_path, reward_path)\n",
    "    global_model = ActorCritic(env.observation_space.shape[0], env.action_space.n)\n",
    "    global_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    workers = [Worker(env, global_model, global_optimizer, gamma, i, max_episodes, batch_size) for i in range(num_workers)]\n",
    "    for worker in workers:\n",
    "        worker.start()\n",
    "\n",
    "    for worker in workers:\n",
    "        worker.join()\n",
    "\n",
    "\n",
    "gps_data_path = \"/content/gdrive/MyDrive/RL/Dichuyen/DaLoc/DaLoc.npy\"\n",
    "reward_data_path = \"/content/gdrive/MyDrive/RL/Dichuyen/Reward/Reward.npy\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_a3c(gps_data_path, reward_data_path, max_episodes=500, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSCYlqGOF6FN",
    "outputId": "2fb68697-b8ce-4167-850b-f015f51fd471"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom Environment\n",
    "class LocationEnv(Env):\n",
    "    def __init__(self, npy_file_path, reward_file_path):\n",
    "        self.route_data = np.load(npy_file_path)  # Route coordinates\n",
    "        self.target_location = np.load(reward_file_path)  # Target coordinates\n",
    "        self.current_index = 0\n",
    "\n",
    "        self.action_space = Discrete(3)  # Actions: Move Forward, Move Backward, Stay in Place\n",
    "        low = np.min(self.route_data, axis=0)\n",
    "        high = np.max(self.route_data, axis=0)\n",
    "        self.observation_space = Box(low=low, high=high, dtype=np.float32)  # Observation: Current position (longitude, latitude)\n",
    "\n",
    "        self.state = np.copy(self.route_data[self.current_index])  # Initial state\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_index = 0\n",
    "        self.state = np.copy(self.route_data[self.current_index])\n",
    "        return np.expand_dims(self.state, axis=0)  # Adding batch dimension\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 1:  # Move forward\n",
    "            if self.current_index < len(self.route_data) - 1:\n",
    "                self.current_index += 1\n",
    "        elif action == 0:  # Move backward\n",
    "            if self.current_index > 0:\n",
    "                self.current_index -= 1\n",
    "        elif action == 2:  # Stay in place\n",
    "            pass\n",
    "\n",
    "        self.state = np.copy(self.route_data[self.current_index])\n",
    "\n",
    "        # Calculate the reward based on the distance to the target location\n",
    "        distance = np.linalg.norm(self.state - self.target_location)\n",
    "        reward = -distance  # Negative reward as the agent is penalized the further it is from the target\n",
    "\n",
    "        # Check if the agent has reached the target\n",
    "        done = distance < 0.001 or self.current_index == len(self.route_data) - 1\n",
    "\n",
    "        if done:\n",
    "            reward = 10  # Positive reward for reaching the target\n",
    "\n",
    "        return np.expand_dims(self.state, axis=0), reward, done, {\"distance\": distance}\n",
    "\n",
    "    def render(self):\n",
    "        # Visualize the current route and the target location\n",
    "        plt.plot(self.route_data[:, 0], self.route_data[:, 1], label='Route', color='blue')\n",
    "        plt.scatter(self.state[0], self.state[1], color='red', label='Current Location')\n",
    "        plt.scatter(self.target_location[0], self.target_location[1], color='green', label='Target Location')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        plt.title('GPS Path Visualization')\n",
    "        plt.show()\n",
    "\n",
    "# Actor-Critic Network (Shared Network)\n",
    "class ActorCritic(Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.shared = layers.Dense(128, activation=\"relu\")\n",
    "        self.actor = layers.Dense(action_size, activation=\"softmax\")  # Action probabilities\n",
    "        self.critic = layers.Dense(1)  # State value estimation\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.shared(inputs)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "# Worker class for A3C (Asynchronous Advantage Actor-Critic)\n",
    "class Worker(threading.Thread):\n",
    "    def __init__(self, env, global_model, optimizer, gamma, worker_id, max_episodes, batch_size):\n",
    "        super(Worker, self).__init__()\n",
    "        self.env = env\n",
    "        self.global_model = global_model\n",
    "        self.optimizer = optimizer\n",
    "        self.gamma = gamma\n",
    "        self.worker_id = worker_id\n",
    "        self.max_episodes = max_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.local_model = ActorCritic(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "        # Ensure that the local model is built with correct input shape before copying weights\n",
    "        self.local_model.build(input_shape=(None, env.observation_space.shape[0]))  # Build model with correct input shape\n",
    "        self.local_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))  # Compile to initialize weights\n",
    "\n",
    "        # Initialize weights from global model\n",
    "        self.local_model.set_weights(self.global_model.get_weights())\n",
    "\n",
    "    def run(self):\n",
    "        global_episode = 0\n",
    "        while global_episode < self.max_episodes:\n",
    "            state = self.env.reset()\n",
    "            state = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "            memory = []\n",
    "            total_reward = 0\n",
    "\n",
    "            while True:\n",
    "                logits, value = self.local_model(state)\n",
    "\n",
    "                # Choose action based on probabilities from the actor\n",
    "                action_probabilities = tf.squeeze(logits, axis=0).numpy()\n",
    "                action = np.random.choice(len(action_probabilities), p=action_probabilities)\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                memory.append((state, action, reward, value))\n",
    "\n",
    "                state = tf.convert_to_tensor(next_state, dtype=tf.float32)\n",
    "                if done:\n",
    "                    self.update_global(memory, done)\n",
    "                    break\n",
    "\n",
    "            print(f\"Worker {self.worker_id}, Episode Reward: {total_reward}\")\n",
    "            global_episode += 1\n",
    "\n",
    "    def update_global(self, memory, done):\n",
    "        states, actions, rewards, values = zip(*memory)\n",
    "\n",
    "        # Apply batch size (use batch_size for each update)\n",
    "        returns = []\n",
    "        discounted_sum = 0 if done else values[-1].numpy()\n",
    "        for reward in reversed(rewards):\n",
    "            discounted_sum = reward + self.gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        returns = tf.convert_to_tensor(returns, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        values = tf.squeeze(tf.convert_to_tensor(values), axis=1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits, critic_values = self.local_model(tf.concat(states, axis=0))  # Concatenating states to form batch\n",
    "            critic_loss = tf.reduce_mean(tf.square(returns - critic_values))\n",
    "            actor_loss = -tf.reduce_mean((returns - critic_values) * logits)\n",
    "            total_loss = critic_loss + actor_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.local_model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.global_model.trainable_weights))\n",
    "\n",
    "        # Update global model with the local model's weights after training\n",
    "        self.global_model.set_weights(self.local_model.get_weights())\n",
    "\n",
    "# Main A3C Training\n",
    "def train_a3c(env_path, reward_path, max_episodes=500, num_workers=4, gamma=0.99, batch_size=32):\n",
    "    env = LocationEnv(env_path, reward_path)\n",
    "    global_model = ActorCritic(env.observation_space.shape[0], env.action_space.n)\n",
    "    dummy_state = tf.random.uniform((1, env.observation_space.shape[0])) # Create a dummy state\n",
    "    _ = global_model(dummy_state)\n",
    "    global_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    workers = [Worker(env, global_model, global_optimizer, gamma, i, max_episodes, batch_size) for i in range(num_workers)]\n",
    "    for worker in workers:\n",
    "        worker.start()\n",
    "\n",
    "    for worker in workers:\n",
    "        worker.join()\n",
    "\n",
    "gps_data_path = \"/content/gdrive/MyDrive/RL/Dichuyen/DaLoc/DaLoc.npy\"\n",
    "reward_data_path = \"/content/gdrive/MyDrive/RL/Dichuyen/Reward/Reward.npy\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_a3c(gps_data_path, reward_data_path, max_episodes=500, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ht2TWfc_F-Z4",
    "outputId": "115401bd-6981-411f-b2f1-a6c20aeb095a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import threading\n",
    "\n",
    "class ActorCritic(Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.shared = layers.Dense(128, activation=\"relu\")  # Using 128 neurons for hidden layer\n",
    "        self.actor = layers.Dense(action_size, activation=\"softmax\")  # Action probabilities\n",
    "        self.critic = layers.Dense(1)  # State value estimation\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.shared(inputs)\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "class Worker(threading.Thread):\n",
    "    def __init__(self, env, global_model, optimizer, gamma, worker_id, max_episodes, batch_size):\n",
    "        super(Worker, self).__init__()\n",
    "        self.env = env\n",
    "        self.global_model = global_model\n",
    "        self.optimizer = optimizer\n",
    "        self.gamma = gamma\n",
    "        self.worker_id = worker_id\n",
    "        self.max_episodes = max_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.local_model = ActorCritic(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "        # Initialize model and set weights from global model\n",
    "        self.local_model.build(input_shape=(None, env.observation_space.shape[0]))\n",
    "        self.local_model.compile(optimizer=Adam(learning_rate=0.0001))  # Adam optimizer with a learning rate of 1e-4\n",
    "        self.local_model.set_weights(self.global_model.get_weights())\n",
    "\n",
    "    def run(self):\n",
    "        global_episode = 0\n",
    "        while global_episode < self.max_episodes:\n",
    "            state = self.env.reset()\n",
    "            state = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "            memory = []\n",
    "            total_reward = 0\n",
    "\n",
    "            while True:\n",
    "                logits, value = self.local_model(state)\n",
    "\n",
    "                action_probabilities = tf.squeeze(logits, axis=0).numpy()\n",
    "                action = np.random.choice(len(action_probabilities), p=action_probabilities)\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                memory.append((state, action, reward, value))\n",
    "\n",
    "                state = tf.convert_to_tensor(next_state, dtype=tf.float32)\n",
    "                if done:\n",
    "                    self.update_global(memory, done)\n",
    "                    break\n",
    "\n",
    "            print(f\"Worker {self.worker_id}, Episode Reward: {total_reward}\")\n",
    "            global_episode += 1\n",
    "\n",
    "    def update_global(self, memory, done):\n",
    "        states, actions, rewards, values = zip(*memory)\n",
    "\n",
    "        returns = []\n",
    "        discounted_sum = 0 if done else values[-1].numpy()\n",
    "        for reward in reversed(rewards):\n",
    "            discounted_sum = reward + self.gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        returns = tf.convert_to_tensor(returns, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        values = tf.squeeze(tf.convert_to_tensor(values), axis=1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits, critic_values = self.local_model(tf.concat(states, axis=0))\n",
    "            critic_loss = tf.reduce_mean(tf.square(returns - critic_values))\n",
    "            actor_loss = -tf.reduce_mean((returns - critic_values) * logits)\n",
    "            total_loss = critic_loss + actor_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.local_model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.global_model.trainable_weights))\n",
    "\n",
    "        self.global_model.set_weights(self.local_model.get_weights())\n",
    "\n",
    "def train_a3c(env_path, reward_path, max_episodes=500, num_workers=4, gamma=0.99, batch_size=32):\n",
    "    env = LocationEnv(env_path, reward_path)\n",
    "    global_model = ActorCritic(env.observation_space.shape[0], env.action_space.n)\n",
    "    dummy_state = tf.random.uniform((1, env.observation_space.shape[0]))\n",
    "    _ = global_model(dummy_state)\n",
    "    global_optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "    workers = [Worker(env, global_model, global_optimizer, gamma, i, max_episodes, batch_size) for i in range(num_workers)]\n",
    "    for worker in workers:\n",
    "        worker.start()\n",
    "\n",
    "    for worker in workers:\n",
    "        worker.join()\n",
    "\n",
    "# Train the model\n",
    "gps_data_path = \"/content/gdrive/MyDrive/RL/Dichuyen/DaLoc/DaLoc.npy\"\n",
    "reward_data_path = \"/content/gdrive/MyDrive/RL/Dichuyen/Reward/Reward.npy\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_a3c(gps_data_path, reward_data_path, max_episodes=500, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6_LX7f0xezy"
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAJ4TJ_a3cLL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fs7vXm4ZrdRc",
    "outputId": "3619b9a7-6f6e-44d2-ead2-6ed49f83a63c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Hàm đánh giá các mô hình (Precision, Recall, F1, MSE, RMSE, Purity, Silhouette Score)\n",
    "def evaluate_model(true_values, predicted_values, action_space, cluster_data=None):\n",
    "    \"\"\"\n",
    "    Đánh giá mô hình với các tiêu chí Precision, Recall, F1, MSE, RMSE, Purity, Silhouette Score.\n",
    "\n",
    "    :param true_values: Mảng các giá trị thực tế\n",
    "    :param predicted_values: Mảng các giá trị dự đoán từ mô hình\n",
    "    :param action_space: Kích thước không gian hành động\n",
    "    :param cluster_data: Dữ liệu dùng để tính Purity và Silhouette nếu có\n",
    "    :return: Dictionary với các giá trị đánh giá\n",
    "    \"\"\"\n",
    "    # Tính Precision, Recall, F1\n",
    "    precision = precision_score(true_values, predicted_values, average='micro')\n",
    "    recall = recall_score(true_values, predicted_values, average='micro')\n",
    "    f1_micro = f1_score(true_values, predicted_values, average='micro')\n",
    "    f1_macro = f1_score(true_values, predicted_values, average='macro')\n",
    "\n",
    "    # Tính MSE và RMSE\n",
    "    mse = mean_squared_error(true_values, predicted_values)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # Tính Purity nếu có cluster_data (dữ liệu phân cụm)\n",
    "    if cluster_data is not None:\n",
    "        le = LabelEncoder()\n",
    "        true_labels = le.fit_transform(true_values)\n",
    "        predicted_labels = le.transform(predicted_values)\n",
    "        # Tính purity bằng cách đo tỉ lệ đúng của các điểm trong cụm\n",
    "        purity = purity_score(true_labels, predicted_labels)\n",
    "\n",
    "        # Tính Silhouette Score nếu có dữ liệu phân cụm\n",
    "        silhouette = silhouette_score(cluster_data, predicted_labels)\n",
    "    else:\n",
    "        purity = None\n",
    "        silhouette = None\n",
    "\n",
    "    # Trả về kết quả đánh giá dưới dạng dictionary\n",
    "    evaluation_results = {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Micro\": f1_micro,\n",
    "        \"F1 Macro\": f1_macro,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Purity\": purity,\n",
    "        \"Silhouette Score\": silhouette\n",
    "    }\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "# Hàm tính Purity\n",
    "def purity_score(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Tính Purity giữa true_labels và predicted_labels.\n",
    "    Purity là tỷ lệ điểm dữ liệu được phân vào cụm đúng.\n",
    "\n",
    "    :param true_labels: Mảng các nhãn thực tế\n",
    "    :param predicted_labels: Mảng các nhãn dự đoán\n",
    "    :return: Purity score\n",
    "    \"\"\"\n",
    "    # Đếm số lượng điểm đúng trong các cụm\n",
    "    contingency_matrix = np.zeros((len(np.unique(true_labels)), len(np.unique(predicted_labels))))\n",
    "    for i in range(len(true_labels)):\n",
    "        contingency_matrix[true_labels[i], predicted_labels[i]] += 1\n",
    "\n",
    "    # Tính Purity\n",
    "    purity = np.sum(np.max(contingency_matrix, axis=0)) / len(true_labels)\n",
    "    return purity\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "if __name__ == \"__main__\":\n",
    "    # Giả sử true_values là giá trị thực tế (ví dụ vị trí GPS thực tế) và predicted_values là giá trị dự đoán của mô hình\n",
    "    true_values = np.array([0, 1, 0, 2, 1, 2, 0, 1, 2, 0])  # Ví dụ cho hành động\n",
    "    predicted_values = np.array([0, 1, 0, 2, 1, 1, 0, 2, 2, 0])  # Dự đoán mô hình\n",
    "\n",
    "    # Đánh giá mô hình với các chỉ số\n",
    "    results = evaluate_model(true_values, predicted_values, action_space=3)\n",
    "\n",
    "    # In kết quả\n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "tgR-6SzCWtvV",
    "outputId": "a4ba16f1-da17-41fa-da53-13455e3b389d"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data for comparison\n",
    "data = {\n",
    "    'A3C': [0.8, 0.8, 0.8, 0.78, 0.15, 0.387, 150],\n",
    "    'DQN': [0.78, 0.78, 0.78, 0.76, 0.2, 0.447, 140],\n",
    "    'DDQN': [0.79, 0.79, 0.79, 0.77, 0.18, 0.424, 145],\n",
    "    'Q-learning': [0.75, 0.74, 0.75, 0.73, 0.25, 0.5, 130]\n",
    "}\n",
    "\n",
    "# Define the labels for the metrics\n",
    "metrics = ['Precision', 'Recall', 'F1 Micro', 'F1 Macro', 'MSE', 'RMSE', 'Cumulative Reward']\n",
    "\n",
    "# Convert data to a numpy array\n",
    "values = np.array(list(data.values()))\n",
    "\n",
    "# Create the heatmap\n",
    "sns.heatmap(values, annot=True, cmap='Blues', xticklabels=metrics, yticklabels=data.keys(), fmt='.2f')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Algorithms')\n",
    "plt.title('Performance Comparison between Algorithms')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "bJfHP_yxbHka",
    "outputId": "991b3864-f242-4aa5-ef79-8a4abbbaed67"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Dữ liệu so sánh các tham số giữa các phương pháp\n",
    "data = {\n",
    "    'Learning Rate': [0.001, 0.001, 0.001, 0.01],\n",
    "    'Gamma': [0.99, 0.99, 0.99, 0.99],\n",
    "    'Số Worker': [4, 1, 1, 1],\n",
    "    'Batch Size': [32, 32, 32, 1],\n",
    "    'Max Episodes': [500, 1000, 1000, 1000],\n",
    "}\n",
    "\n",
    "# Tên các phương pháp\n",
    "methods = ['A3C', 'DQN', 'DDQN', 'Q-Learning']\n",
    "\n",
    "# Tạo biểu đồ cột cho mỗi tham số\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "\n",
    "# Vẽ các biểu đồ cột cho các tham số\n",
    "for i, (param, values) in enumerate(data.items()):\n",
    "    axes[i].bar(methods, values, color=['blue', 'orange', 'green', 'red'])\n",
    "    axes[i].set_title(param)\n",
    "    axes\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
